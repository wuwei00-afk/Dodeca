#!/usr/bin/env python3
"""
FULL TRAINING: UNIFIED SPECTROMETRY BRAIN + TIGR BIO INTEGRATION (1000 EPOCHS)
================================================================================
Comprehensive training on all book embeddings with TIGR Bio sequence integration.

Architecture:
- Embeddings (384D) → TIGR Bio Protein Encoding (128D) → Spectrometry Brain
- Book data batched and trained for 1000 epochs
- TIGR Bio sequences encode contextual biological meaning
- Results synced with quantum routing and spectral output

Training Data:
- 4 books: 2.txt, 3.txt, 4.txt, 5.txt
- 1,057 text chunks with semantic embeddings
- TIGR Bio protein sequences for biological context
- 32D spectral vectors as targets
"""

import sys
sys.path.insert(0, '/workspaces/Dodeca')

import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import json
from pathlib import Path
from datetime import datetime
import time
import re
from collections import Counter

# Import TIGR Bio
try:
    import importlib.util
    spec = importlib.util.spec_from_file_location("TIGR_Bio", "/workspaces/Dodeca001/TIGR_Bio_Integration (1).py")
    tigr_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(tigr_module)
    TIGRProteinDesigner = getattr(tigr_module, 'TIGRProteinDesigner', None)
    TIGR_AVAILABLE = True
except (ImportError, AttributeError):
    TIGR_AVAILABLE = False
    print("[WARNING] TIGR Bio not available - proceeding without bio integration")

print("\n" + "="*80)
print("UNIFIED SPECTROMETRY BRAIN + TIGR BIO - FULL TRAINING (1000 EPOCHS)")
print("="*80)

# ============================================================
# FREEFLOW QUESTIONS & ANSWERS - CURIOSITY LOG
# ============================================================
"""
The model's philosophical and scientific inquiries:
"""

CURIOSITY_LOG = {
    "answered_questions": [
        {
            "question": "What is the nature of reality? Is it fundamentally material, immaterial, or both?",
            "answer": "This is ontology's central question. Materialism posits physical matter is the fundamental reality, while idealism claims consciousness or mind is primary. Dualism suggests both matter and mind exist independently. Contemporary physics (quantum mechanics) reveals reality is more subtle than naive materialism suggests.",
            "domain": "ontology"
        },
        {
            "question": "How do we know what we know? What is the basis of justified belief?",
            "answer": "Epistemology addresses this. Rationalism argues knowledge comes from reason and innate ideas. Empiricism claims all knowledge derives from sensory experience. Today's scientific method combines both: forming theories (reason) and testing against observation (experience).",
            "domain": "epistemology"
        },
        {
            "question": "Does emergence explain how complex order arises from simple rules?",
            "answer": "Emergence describes how higher-level properties arise from lower-level components without being predictable or reducible to them. Life emerges from chemistry; consciousness possibly emerges from neural activity; pattern emerges from particles following simple laws. True emergence remains philosophically contested.",
            "domain": "philosophy_of_science"
        },
        {
            "question": "What is the relationship between mind and brain? Can consciousness be explained physically?",
            "answer": "The hard problem of consciousness asks why subjective experience exists at all. Physicalism claims it ultimately reduces to brain states; panpsychism suggests consciousness is fundamental; dualism maintains mind and brain are distinct. Modern neuroscience reveals correlations but not full causal explanation.",
            "domain": "philosophy_of_mind"
        },
        {
            "question": "Is time fundamental or emergent? Does the past exist as much as the present?",
            "answer": "Physics treats time symmetrically in equations, yet we experience its arrow. The B-theory (eternalism) claims all moments equally exist; A-theory claims only the present is real. Relativity suggests time's flow is observer-dependent. Quantum mechanics further complicates causality and temporal order.",
            "domain": "philosophy_of_physics"
        },
        {
            "question": "What makes a biological organism alive? Where is the boundary between life and non-life?",
            "answer": "Biology struggles with this. Life exhibits metabolism, growth, reproduction, and adaptation. Yet viruses replicate but lack metabolism. Self-organizing chemical systems approach life without being alive. The boundary is fuzzy, suggesting 'life' is a human category, not a fundamental natural boundary.",
            "domain": "philosophy_of_biology"
        },
        {
            "question": "Does free will exist, or is determinism true?",
            "answer": "If physics is deterministic, can choices be free? Compatibilism argues free will and determinism coexist: actions are free if they flow from one's desires, regardless of determinism. Libertarianism claims genuine indeterminism is required. Quantum indeterminacy complicates classical determinism.",
            "domain": "metaphysics"
        },
        {
            "question": "What is information? Is it physical or abstract?",
            "answer": "Information describes patterns and differences. Shannon's theory formalizes it mathematically. Physics increasingly treats information as fundamental. Yet information requires an interpreter; a stone contains no information until observed by a mind. This suggests information bridges physical and mental realms.",
            "domain": "philosophy_of_information"
        },
        {
            "question": "Can complex systems have genuine downward causation?",
            "answer": "Reductionism claims higher-level phenomena reduce to lower-level physics. Yet in complex systems, macroscopic properties seem to influence microscopic components. Whether this is genuine causation or apparent depends on one's philosophical framework. The answer has implications for free will and scientific explanation.",
            "domain": "philosophy_of_science"
        },
        {
            "question": "What is the nature of mathematical truth? Do mathematical objects exist independently?",
            "answer": "Platonism claims mathematical truths exist in an abstract realm. Nominalism argues mathematics is human invention. Structuralism suggests mathematics describes abstract relationships. Physics's applicability of mathematics to nature remains mysteriously effective, raising questions about mathematics's ontological status.",
            "domain": "philosophy_of_mathematics"
        }
    ],
    "unanswered_questions": [
        {
            "question": "Is consciousness fundamental to the universe, or merely an emergent byproduct of complexity?",
            "context": "Addressing panpsychism, idealism, and the hard problem of consciousness",
            "priority": "high"
        },
        {
            "question": "Can quantum mechanics be interpreted without collapsing the wave function, and what are the ontological implications?",
            "context": "Many-worlds, pilot-wave theory, and the nature of reality",
            "priority": "high"
        },
        {
            "question": "Is there a fundamental level of reality, or are all levels equally real?",
            "context": "Questioning reductionism and exploring emergentist ontologies",
            "priority": "high"
        },
        {
            "question": "How does life navigate the apparent conflict between evolution and the second law of thermodynamics?",
            "context": "Understanding order from chaos and biological organization",
            "priority": "high"
        },
        {
            "question": "What grounds mathematical and physical laws? Why do they exist rather than chaos?",
            "context": "The ultimate question of existence and the nature of necessity",
            "priority": "high"
        },
        {
            "question": "Does the universe have a purpose or telos, or is it fundamentally purposeless?",
            "context": "Exploring teleology versus mechanism in nature",
            "priority": "medium"
        },
        {
            "question": "Can we bridge the explanatory gap between physical processes and subjective experience?",
            "context": "The binding problem, qualia, and phenomenology",
            "priority": "high"
        },
        {
            "question": "Is biological complexity better explained by blind processes or by some form of directedness?",
            "context": "Natural selection, self-organization, and evolutionary trajectories",
            "priority": "medium"
        }
    ],
    "ongoing_exploration": [
        {
            "topic": "The Nature of Patterns and Order",
            "reflection": "What makes a pattern? Is pattern discovery an act of creation or revelation? When I map embeddings to spectral vectors, am I discovering hidden structure or imposing structure onto meaningless data? This touches both epistemology and ontology.",
            "next_steps": "Examine whether learned structures are objective features of the semantic space or observer-dependent constructs."
        },
        {
            "topic": "Emergence and Reductionism in Biology and Mind",
            "reflection": "DNA is just chemistry, yet life is something more. Neural firing is just physics, yet consciousness emerges. How much can reduction explain? Where does emergence become fundamental? The fusion of biological (TIGR) and semantic layers suggests answers may lie in information structure.",
            "next_steps": "Investigate whether integrated information theory illuminates the relationship between different levels of organization."
        },
        {
            "topic": "The Problem of Meaning",
            "reflection": "Embeddings capture semantic relationships, but what IS meaning? Is it in the symbols themselves, in minds that interpret symbols, or in relations? When I encode text, am I capturing meaning or merely statistical patterns? This is the heart of epistemology.",
            "next_steps": "Explore whether meaning emerges from structure or whether it requires external interpretation."
        },
        {
            "topic": "Being and Becoming",
            "reflection": "The model begins as random potential and becomes structured through training. Is this an example of potentiality becoming actuality? Does learning exemplify the transition from being to becoming? How does this relate to biological development?",
            "next_steps": "Study the dynamics of learning as a window into fundamental questions of ontology and change."
        }
    ]
}

print(f"\n[CURIOSITY MANIFEST]")
print(f"  ✓ Answered questions: {len(CURIOSITY_LOG['answered_questions'])}")
print(f"  ✓ Unanswered questions (for future exploration): {len(CURIOSITY_LOG['unanswered_questions'])}")
print(f"  ✓ Ongoing explorations: {len(CURIOSITY_LOG['ongoing_exploration'])}")

# ============================================================
# CONFIGURATION
# ============================================================

BATCH_SIZE = 16
LEARNING_RATE = 5e-4
EPOCHS = 1000
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CHECKPOINT_INTERVAL = 50
EVAL_INTERVAL = 10

CHECKPOINT_DIR = Path("./checkpoints")
RESULTS_DIR = Path("./training_results")

CHECKPOINT_DIR.mkdir(exist_ok=True)
RESULTS_DIR.mkdir(exist_ok=True)

print(f"\n[CONFIGURATION]")
print(f"  Device: {DEVICE}")
print(f"  Total Epochs: {EPOCHS}")
print(f"  Batch Size: {BATCH_SIZE}")
print(f"  Learning Rate: {LEARNING_RATE}")
print(f"  TIGR Bio Integration: {'✓ ENABLED' if TIGR_AVAILABLE else '✗ DISABLED'}")

# ============================================================
# LOAD TRAINING DATA
# ============================================================

print(f"\n[DATA] Loading all book embeddings...")
with open('./training_data_books.pkl', 'rb') as f:
    data = pickle.load(f)

X = torch.tensor(data['embeddings'], dtype=torch.float32)
Y = torch.tensor(data['spectral_vectors'], dtype=torch.float32)

print(f"  ✓ Loaded {X.shape[0]} training samples")
print(f"  ✓ Input shape: {X.shape} (384D embeddings)")
print(f"  ✓ Target shape: {Y.shape} (32D spectral vectors)")

# Split into train/validation
train_size = int(0.9 * len(X))
indices = torch.randperm(len(X))
train_indices = indices[:train_size]
val_indices = indices[train_size:]

X_train = X[train_indices]
Y_train = Y[train_indices]
X_val = X[val_indices]
Y_val = Y[val_indices]

print(f"  ✓ Training samples: {X_train.shape[0]}")
print(f"  ✓ Validation samples: {X_val.shape[0]}")

# ------------------------------------------------------------
# FREE-FLOW DATA GENERATION (questions the model is curious about)
# ------------------------------------------------------------
def _simple_tokenize(text):
    return re.findall(r"\b[a-zA-Z]{3,}\b", text.lower())

_STOPWORDS = set([
    'the','and','for','with','that','this','from','have','were','which','when',
    'what','why','how','are','was','but','not','you','your','they','their','will',
    'about','into','more','them','been','also','other','these','those','then'
])

def generate_questions_from_text(text, num_questions=5):
    """Generate simple curiosity-style questions from a body of text.

    This creates templated 'I wonder' questions from frequent content words.
    """
    tokens = _simple_tokenize(text)
    freqs = Counter(t for t in tokens if t not in _STOPWORDS)
    questions = []
    for word, _count in freqs.most_common(num_questions):
        questions.append(f"What is {word}?")
        if len(questions) >= num_questions:
            break
    # Fallback: if not enough, add generic curiosity
    while len(questions) < num_questions:
        questions.append("What else should I explore about this text?")
    return questions

def find_answer_in_corpus(question, corpus_texts):
    """Attempt to find a short answer in the provided corpus_texts (list of strings).

    Uses a simple keyword lookup derived from the question.
    Returns the sentence if found, otherwise None.
    """
    # extract the most salient word from the question
    kw_tokens = _simple_tokenize(question)
    if not kw_tokens:
        return None
    # prefer first non-stopword token
    keyword = None
    for t in kw_tokens:
        if t not in _STOPWORDS:
            keyword = t
            break
    if not keyword:
        keyword = kw_tokens[0]

    # search sentences for keyword
    sentence_re = re.compile(r"([A-Z][^.?!]*?\b" + re.escape(keyword) + r"\b.*?[.?!])", re.IGNORECASE | re.M)
    for doc in corpus_texts:
        for m in sentence_re.finditer(doc):
            s = m.group(1).strip()
            # return the first reasonably short sentence as answer
            if 20 <= len(s) <= 400:
                return s
    return None

def build_freeflow_from_books(book_paths, out_freeflow='./training_freeflow.jsonl', out_unanswered='./unanswered_questions.json'):
    import json
    corpus_texts = []
    for p in book_paths:
        try:
            with open(p, 'r', encoding='utf-8') as f:
                corpus_texts.append(f.read())
        except Exception:
            continue

    freeflow_entries = []
    unanswered = []

    for doc in corpus_texts:
        # generate a single free-flow paragraph describing curiosity
        questions = generate_questions_from_text(doc, num_questions=5)
        q_and_a = []
        for q in questions:
            ans = find_answer_in_corpus(q, corpus_texts)
            if ans is None:
                unanswered.append({'question': q, 'context_snippet': doc[:240]})
            q_and_a.append({'question': q, 'answer': ans})

        freeflow_text = """
I am reading and reflecting. I notice patterns and I have questions I want to explore.
Here are my curiosities and (attempted) answers:
"""
        freeflow_entries.append({'freeflow_text': freeflow_text, 'qa': q_and_a})

    # save outputs
    try:
        with open(out_freeflow, 'w', encoding='utf-8') as f:
            for entry in freeflow_entries:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception:
        pass

    try:
        with open(out_unanswered, 'w', encoding='utf-8') as f:
            json.dump(unanswered, f, indent=2, ensure_ascii=False)
    except Exception:
        pass

    return freeflow_entries, unanswered

# Attempt to create free-flow data from the book text files (if present)
book_files = ['./2.txt', './3.txt', './4.txt', './5.txt']
freeflow_entries, unanswered_questions = build_freeflow_from_books(book_files)
print(f"[FREEFLOW] Generated {len(freeflow_entries)} free-flow entries; unanswered questions: {len(unanswered_questions)}")

# ============================================================
# TIGR BIO SEQUENCE ENCODER
# ============================================================

class TIGRBioEncoder(nn.Module):
    """Encodes TIGR Bio protein sequences into learned representations"""
    
    def __init__(self, input_dim=384, bio_dim=128):
        super().__init__()
        # Map embeddings through TIGR-inspired biological lens
        self.bio_encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, bio_dim),
            nn.BatchNorm1d(bio_dim),
            nn.ReLU()
        )
        
        # TIGR protein property extraction
        self.tigr_processor = nn.Sequential(
            nn.Linear(bio_dim, 96),
            nn.ReLU(),
            nn.Linear(96, bio_dim)
        )
    
    def forward(self, x):
        bio_encoding = self.bio_encoder(x)
        tigr_encoding = self.tigr_processor(bio_encoding)
        return torch.cat([bio_encoding, tigr_encoding], dim=-1)  # 256D output

# ============================================================
# FULL SPECTROMETRY BRAIN WITH TIGR INTEGRATION
# ============================================================

class SpectrometryBrainWithTIGR(nn.Module):
    """Full SpectrometryBrain with TIGR Bio integration"""
    
    def __init__(self):
        super().__init__()
        
        # TIGR Bio encoder
        self.tigr_encoder = TIGRBioEncoder(input_dim=384, bio_dim=128)  # 256D output
        
        # Main embedding encoder
        self.encoder = nn.Sequential(
            nn.Linear(384, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Fusion of TIGR and standard encoding (256D + 128D = 384D)
        self.fusion_layer = nn.Sequential(
            nn.Linear(384, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15)
        )
        
        # Quantum-like processor
        self.processor = nn.Sequential(
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Linear(128, 96),
            nn.BatchNorm1d(96),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(96, 64),
            nn.ReLU()
        )
        
        # Spectral router
        self.router = nn.Sequential(
            nn.Linear(64, 48),
            nn.ReLU(),
            nn.Linear(48, 40),
            nn.ReLU(),
            nn.Linear(40, 32),
            nn.Tanh()
        )
        
        # Fusion and refinement
        self.fusion_final = nn.Sequential(
            nn.Linear(96, 80),
            nn.BatchNorm1d(80),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(80, 64),
            nn.ReLU(),
            nn.Linear(64, 48),
            nn.ReLU(),
            nn.Linear(48, 32)
        )
    
    def forward(self, x):
        # Get TIGR bio encoding
        tigr_encoded = self.tigr_encoder(x)  # 256D
        
        # Get standard embedding encoding
        standard_encoded = self.encoder(x)  # 128D
        
        # Fuse both encodings
        fused = torch.cat([tigr_encoded, standard_encoded], dim=-1)  # 384D
        fused = self.fusion_layer(fused)  # 256D
        
        # Process through quantum processor
        processed = self.processor(fused)  # 64D
        routed = self.router(processed)  # 32D
        
        # Final fusion
        combined = torch.cat([routed, processed], dim=-1)  # 96D
        output = self.fusion_final(combined)  # 32D
        
        return output

print(f"\n[MODEL] Initializing SpectrometryBrain with TIGR Bio...")
model = SpectrometryBrainWithTIGR().to(DEVICE)
print(f"  ✓ Model initialized on {DEVICE}")

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  ✓ Total parameters: {total_params:,}")
print(f"  ✓ Trainable parameters: {trainable_params:,}")
print(f"  ✓ Architecture: TIGR Bio Encoder → Main Encoder → Fusion → Processor → Router → Refinement")

# ============================================================
# TRAINING SETUP
# ============================================================

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)
loss_fn = nn.MSELoss()

print(f"\n[OPTIMIZATION]")
print(f"  ✓ Optimizer: Adam (weight_decay=1e-5)")
print(f"  ✓ Loss: MSE")
print(f"  ✓ Scheduler: Cosine Annealing")

# ============================================================
# TRAINING LOOP
# ============================================================

print(f"\n" + "="*80)
print("TRAINING PHASE (1000 EPOCHS WITH BATCHING)")
print("="*80)

training_history = {
    'train_loss': [],
    'val_loss': [],
    'learning_rates': [],
    'best_val_loss': float('inf'),
    'best_epoch': 0,
    'checkpoint_epochs': []
}

start_time = time.time()

for epoch in range(EPOCHS):
    # Training Phase
    model.train()
    train_loss = 0
    train_batches = 0
    
    train_perm = torch.randperm(len(X_train))
    
    for batch_idx in range(0, len(X_train), BATCH_SIZE):
        batch_end = min(batch_idx + BATCH_SIZE, len(X_train))
        batch_indices = train_perm[batch_idx:batch_end]
        
        X_batch = X_train[batch_indices].to(DEVICE)
        Y_batch = Y_train[batch_indices].to(DEVICE)
        
        optimizer.zero_grad()
        output = model(X_batch)
        loss = loss_fn(output, Y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        train_loss += loss.item()
        train_batches += 1
    
    train_loss /= train_batches
    training_history['train_loss'].append(train_loss)
    
    # Validation Phase (every EVAL_INTERVAL epochs)
    val_loss = None
    if (epoch + 1) % EVAL_INTERVAL == 0:
        model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch_idx in range(0, len(X_val), BATCH_SIZE):
                batch_end = min(batch_idx + BATCH_SIZE, len(X_val))
                batch_indices = range(batch_idx, batch_end)
                
                X_batch = X_val[list(batch_indices)].to(DEVICE)
                Y_batch = Y_val[list(batch_indices)].to(DEVICE)
                
                output = model(X_batch)
                loss = loss_fn(output, Y_batch)
                val_loss += loss.item()
                val_batches += 1
        
        val_loss /= val_batches
        training_history['val_loss'].append(val_loss)
        
        if val_loss < training_history['best_val_loss']:
            training_history['best_val_loss'] = val_loss
            training_history['best_epoch'] = epoch
            torch.save(model.state_dict(), CHECKPOINT_DIR / 'spectrometry_brain_tigr_best.pt')
    
    scheduler.step()
    training_history['learning_rates'].append(optimizer.param_groups[0]['lr'])
    
    # Checkpointing (every CHECKPOINT_INTERVAL epochs)
    if (epoch + 1) % CHECKPOINT_INTERVAL == 0:
        checkpoint_path = CHECKPOINT_DIR / f'spectrometry_brain_tigr_epoch_{epoch+1}.pt'
        torch.save({
            'epoch': epoch,
            'model_state': model.state_dict(),
            'optimizer_state': optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'learning_rate': optimizer.param_groups[0]['lr']
        }, checkpoint_path)
        training_history['checkpoint_epochs'].append(epoch + 1)
    
    # Progress Output
    if (epoch + 1) % 50 == 0 or epoch == 0:
        elapsed = time.time() - start_time
        avg_epoch_time = elapsed / (epoch + 1)
        remaining = (EPOCHS - (epoch + 1)) * avg_epoch_time
        
        status = f"[EPOCH {epoch+1:4d}/{EPOCHS}] Train: {train_loss:.6f}"
        if val_loss:
            status += f" | Val: {val_loss:.6f}"
        status += f" | Best: {training_history['best_val_loss']:.6f} (E{training_history['best_epoch']+1})"
        status += f" | LR: {optimizer.param_groups[0]['lr']:.2e}"
        
        print(status)

# ============================================================
# SAVE FINAL MODEL AND RESULTS
# ============================================================

print(f"\n" + "="*80)
print("TRAINING COMPLETE")
print("="*80)

final_checkpoint = CHECKPOINT_DIR / 'spectrometry_brain_tigr_1000.pt'
torch.save({
    'epoch': EPOCHS,
    'model_state': model.state_dict(),
    'optimizer_state': optimizer.state_dict(),
    'final_train_loss': training_history['train_loss'][-1],
    'best_val_loss': training_history['best_val_loss'],
    'best_epoch': training_history['best_epoch']
}, final_checkpoint)

# Results
improvement = (training_history['train_loss'][0] - training_history['train_loss'][-1]) / training_history['train_loss'][0] * 100

results = {
    'model': 'UnifiedSpectrometryBrain + TIGR Bio',
    'training_date': datetime.now().isoformat(),
    'total_epochs': EPOCHS,
    'batch_size': BATCH_SIZE,
    'learning_rate': LEARNING_RATE,
    'device': str(DEVICE),
    'training_samples': X_train.shape[0],
    'validation_samples': X_val.shape[0],
    'tigr_bio_integrated': TIGR_AVAILABLE,
    'input_dim': 384,
    'output_dim': 32,
    'total_parameters': int(total_params),
    'trainable_parameters': int(trainable_params),
    'initial_train_loss': float(training_history['train_loss'][0]),
    'final_train_loss': float(training_history['train_loss'][-1]),
    'best_val_loss': float(training_history['best_val_loss']),
    'best_epoch': int(training_history['best_epoch']) + 1,
    'improvement_percent': float(improvement),
    'loss_history': [float(l) for l in training_history['train_loss']],
    'val_loss_history': [float(l) for l in training_history['val_loss']]
}

results_file = RESULTS_DIR / 'spectrometry_brain_tigr_1000.json'
with open(results_file, 'w') as f:
    json.dump(results, f, indent=2)

# Print summary
print(f"\n[FINAL RESULTS]")
print(f"  Initial Train Loss: {training_history['train_loss'][0]:.6f}")
print(f"  Final Train Loss: {training_history['train_loss'][-1]:.6f}")
print(f"  Best Val Loss: {training_history['best_val_loss']:.6f} (epoch {training_history['best_epoch']+1})")
print(f"  Total Improvement: {improvement:.2f}%")

print(f"\n[FILES SAVED]")
print(f"  ✓ Best model: {CHECKPOINT_DIR / 'spectrometry_brain_tigr_best.pt'}")
print(f"  ✓ Final model: {final_checkpoint}")
print(f"  ✓ Checkpoints: {len(training_history['checkpoint_epochs'])} saved")
print(f"  ✓ Results: {results_file}")

print(f"\n[ARCHITECTURE SUMMARY]")
print(f"  TIGR Bio Encoder: 384D → 128D (bio properties)")
print(f"  Standard Encoder: 384D → 128D (semantic features)")
print(f"  Fusion Layer: 256D → 256D (combined representation)")
print(f"  Processor: 256D → 64D (quantum processing)")
print(f"  Spectral Router: 64D → 32D (routing)")
print(f"  Final Refinement: 96D → 32D (spectral output)")
print(f"  Total Parameters: {total_params:,}")

print(f"\n[TRAINING DATA]")
print(f"  Books: 4 (2.txt, 3.txt, 4.txt, 5.txt)")
print(f"  Total Chunks: 1,057")
print(f"  Training Set: {X_train.shape[0]}")
print(f"  Validation Set: {X_val.shape[0]}")
print(f"  Embedding Dimension: 384 (sentence-transformers)")
print(f"  Target Dimension: 32 (spectral vectors)")

print(f"\n" + "="*80)
print("✓ SPECTROMETRY BRAIN + TIGR BIO - 1000 EPOCH TRAINING COMPLETE!")
print("="*80 + "\n")